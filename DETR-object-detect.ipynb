{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17981458-5ee7-4599-b548-06831e98f2fe",
   "metadata": {},
   "source": [
    "#### Load the Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c724fb0-3d9e-46fa-8fde-1dac8579183f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'detr'...\n",
      "remote: Enumerating objects: 257, done.\u001b[K\n",
      "remote: Total 257 (delta 0), reused 0 (delta 0), pack-reused 257\u001b[K\n",
      "Receiving objects: 100% (257/257), 12.84 MiB | 7.91 MiB/s, done.\n",
      "Resolving deltas: 100% (140/140), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/facebookresearch/detr.git   #cloning github repo of detr to import its unique loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "1465c16c-f1ab-40b0-991e-9412fd8a6d10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "\n",
    "#Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "\n",
    "#sklearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "#CV\n",
    "import cv2\n",
    "\n",
    "################# DETR FUCNTIONS FOR LOSS######################## \n",
    "import sys\n",
    "sys.path.append('./detr/')\n",
    "\n",
    "from detr.models.matcher import HungarianMatcher\n",
    "from detr.models.detr import SetCriterion\n",
    "#################################################################\n",
    "\n",
    "#Albumenatations\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "#Glob\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "237cce42-8052-45ec-b464-0271c5894f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1813a41-f297-4163-bb48-016fc46b47a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "seed = 42\n",
    "num_classes = 2\n",
    "num_queries = 100\n",
    "null_class_coef = 0.5\n",
    "BATCH_SIZE = 8\n",
    "LR = 2e-5\n",
    "EPOCHS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adc44e66-0e0e-41c7-91b3-b2754017bc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a18e3ed8-fc50-4446-99a1-68a484215435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing on datasets\n",
    "marking = pd.read_csv('/Users/yuf/Desktop/Implementation/Computer Vision/global-wheat/train.csv')\n",
    "bboxs = np.stack(marking['bbox'].apply(lambda x: np.fromstring(x[1:-1], sep=',')))\n",
    "for i, column in enumerate(['x', 'y', 'w', 'h']):\n",
    "    marking[column] = bboxs[:,i]\n",
    "marking.drop(columns=['bbox'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd818f99-c202-46f3-86f2-bc59a9392379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>source</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>w</th>\n",
       "      <th>h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>834.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>226.0</td>\n",
       "      <td>548.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>377.0</td>\n",
       "      <td>504.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>834.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>107.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b6ab77fd7</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>usask_1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>117.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147788</th>\n",
       "      <td>5e0747034</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>arvalis_2</td>\n",
       "      <td>64.0</td>\n",
       "      <td>619.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147789</th>\n",
       "      <td>5e0747034</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>arvalis_2</td>\n",
       "      <td>292.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147790</th>\n",
       "      <td>5e0747034</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>arvalis_2</td>\n",
       "      <td>134.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147791</th>\n",
       "      <td>5e0747034</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>arvalis_2</td>\n",
       "      <td>430.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147792</th>\n",
       "      <td>5e0747034</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>arvalis_2</td>\n",
       "      <td>875.0</td>\n",
       "      <td>740.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>61.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147793 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  width  height     source      x      y      w      h\n",
       "0       b6ab77fd7   1024    1024    usask_1  834.0  222.0   56.0   36.0\n",
       "1       b6ab77fd7   1024    1024    usask_1  226.0  548.0  130.0   58.0\n",
       "2       b6ab77fd7   1024    1024    usask_1  377.0  504.0   74.0  160.0\n",
       "3       b6ab77fd7   1024    1024    usask_1  834.0   95.0  109.0  107.0\n",
       "4       b6ab77fd7   1024    1024    usask_1   26.0  144.0  124.0  117.0\n",
       "...           ...    ...     ...        ...    ...    ...    ...    ...\n",
       "147788  5e0747034   1024    1024  arvalis_2   64.0  619.0   84.0   95.0\n",
       "147789  5e0747034   1024    1024  arvalis_2  292.0  549.0  107.0   82.0\n",
       "147790  5e0747034   1024    1024  arvalis_2  134.0  228.0  141.0   71.0\n",
       "147791  5e0747034   1024    1024  arvalis_2  430.0   13.0  184.0   79.0\n",
       "147792  5e0747034   1024    1024  arvalis_2  875.0  740.0   94.0   61.0\n",
       "\n",
       "[147793 rows x 8 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b741fc99-64b6-4986-a211-b1ff8b68a219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuf/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:668: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Creating Folds\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "df_folds = marking[['image_id']].copy()\n",
    "# assemble to get number of bboxes in each image (sum(*) group by image_id)\n",
    "df_folds.loc[:, 'bbox_count'] = 1\n",
    "df_folds = df_folds.groupby('image_id').count()\n",
    "\n",
    "df_folds.loc[:, 'source'] = marking[['image_id', 'source']].groupby('image_id').min()['source']\n",
    "df_folds.loc[:, 'stratify_group'] = np.char.add(\n",
    "    df_folds['source'].values.astype(str),\n",
    "    df_folds['bbox_count'].apply(lambda x: f'_{x // 15}').values.astype(str)\n",
    ")\n",
    "df_folds.loc[:, 'fold'] = 0\n",
    "\n",
    "# intuition: in each cross validation test/train split, distribution of different objects (counts) are similar\n",
    "for fold_number, (train_index, val_index) in enumerate(skf.split(X=df_folds.index, y=df_folds['stratify_group'])):\n",
    "    df_folds.loc[df_folds.iloc[val_index].index, 'fold'] = fold_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b3cbfe3c-b0ce-442b-992f-9c1d5287b18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transforms():\n",
    "    return A.Compose([A.OneOf([A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, val_shift_limit=0.2, p=0.9),\n",
    "                               \n",
    "                      A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.9)],p=0.9),\n",
    "                      \n",
    "                      A.ToGray(p=0.01),\n",
    "                      \n",
    "                      A.HorizontalFlip(p=0.5),\n",
    "                      \n",
    "                      A.VerticalFlip(p=0.5),\n",
    "                      \n",
    "                      A.Resize(height=512, width=512, p=1),\n",
    "                      \n",
    "                      A.Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=0, p=0.5),\n",
    "                      \n",
    "                      ToTensorV2(p=1.0)],\n",
    "                      \n",
    "                      p=1.0,\n",
    "                     \n",
    "                      bbox_params=A.BboxParams(format='coco',min_area=0, min_visibility=0,label_fields=['labels'])\n",
    "                      )\n",
    "\n",
    "def get_valid_transforms():\n",
    "    return A.Compose([A.Resize(height=512, width=512, p=1.0),\n",
    "                      ToTensorV2(p=1.0)], \n",
    "                      p=1.0, \n",
    "                      bbox_params=A.BboxParams(format='coco',min_area=0, min_visibility=0,label_fields=['labels'])\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0372931c-8783-497e-b662-03bd09571f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids=df_train.index.values\n",
    "dataframe=marking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "db33b45e-3c6d-477d-8834-e20aa8901783",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = image_ids[0]\n",
    "records = dataframe[dataframe['image_id']==image_ids[0]]\n",
    "image = cv2.imread(f'{DIR_TRAIN}/{image_id}.jpg', cv2.IMREAD_COLOR)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "image /= 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "15e7c0d3-9359-44c4-96fa-c06c861a6047",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Area of bb\n",
    "area = boxes[:,2]*boxes[:,3]\n",
    "area = torch.as_tensor(area, dtype=torch.float32)\n",
    "\n",
    "# AS pointed out by PRVI It works better if the main class is labelled as zero\n",
    "labels =  np.zeros(len(boxes), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c7345892-0299-4394-b806-9de9f5c17784",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_TRAIN = '/Users/yuf/Desktop/Implementation/Computer Vision/global-wheat/train'\n",
    "\n",
    "class WheatDataset(Dataset):\n",
    "    def __init__(self,image_ids,dataframe,transforms=None):\n",
    "        self.image_ids = image_ids\n",
    "        self.df = dataframe\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return self.image_ids.shape[0]\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        image_id = self.image_ids[index]\n",
    "        records = self.df[self.df['image_id'] == image_id]\n",
    "        \n",
    "        image = cv2.imread(f'{DIR_TRAIN}/{image_id}.jpg', cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        \n",
    "        # DETR takes in data in coco format \n",
    "        boxes = records[['x', 'y', 'w', 'h']].values\n",
    "        \n",
    "        #Area of bb\n",
    "        area = boxes[:,2]*boxes[:,3]\n",
    "        area = torch.as_tensor(area, dtype=torch.float32)\n",
    "        \n",
    "        # AS pointed out by PRVI It works better if the main class is labelled as zero\n",
    "        \"We should change the labels at least to not be all zeros??\"\n",
    "        labels =  np.zeros(len(boxes), dtype=np.int32)\n",
    "\n",
    "        \n",
    "        if self.transforms:\n",
    "            sample = {\n",
    "                'image': image,\n",
    "                'bboxes': boxes,\n",
    "                'labels': labels\n",
    "            }\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "            boxes = sample['bboxes']\n",
    "            labels = sample['labels']\n",
    "            \n",
    "            \n",
    "        #Normalizing BBOXES\n",
    "            \n",
    "        _,h,w = image.shape\n",
    "        boxes = A.augmentations.bbox_utils.normalize_bboxes(sample['bboxes'],rows=h,cols=w)\n",
    "        target = {}\n",
    "        # don't forget to convert everything to tensor\n",
    "        target['boxes'] = torch.as_tensor(boxes,dtype=torch.float32)\n",
    "        target['labels'] = torch.as_tensor(labels,dtype=torch.long)\n",
    "        target['image_id'] = torch.tensor([index])\n",
    "        target['area'] = area\n",
    "        \n",
    "        return image, target, image_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "f300437f-ac61-4e3a-9828-18339679f16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Input images shape: (batch_size, 3, height, weight)\n",
    "where 3 is the three channels (RGB)\n",
    "\n",
    "\"\"\"\n",
    "class DETRModel(nn.Module):\n",
    "    def __init__(self,num_classes,num_queries):\n",
    "        super(DETRModel,self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_queries = num_queries\n",
    "        \n",
    "        self.model = torch.hub.load('facebookresearch/detr', 'detr_resnet50', pretrained=True)\n",
    "        # in_features is 'd_model' for transformer network and here it is fixed to 256 by default\n",
    "        self.in_features = self.model.class_embed.in_features\n",
    "        # change on the 'class_embed' to cope with new num_classes for specific tasks\n",
    "        self.model.class_embed = nn.Linear(in_features=self.in_features,out_features=self.num_classes)\n",
    "        # num_queries is the punultimate dimension for decoder section\n",
    "        self.model.num_queries = self.num_queries\n",
    "        \n",
    "    def forward(self,images):\n",
    "        return self.model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "6d7446ff-d94b-4909-baeb-9462c0119c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "code taken from github repo detr , 'code present in engine.py'\n",
    "'''\n",
    "\n",
    "matcher = HungarianMatcher()\n",
    "\n",
    "weight_dict = weight_dict = {'loss_ce': 1, 'loss_bbox': 1 , 'loss_giou': 1}\n",
    "\n",
    "losses = ['labels', 'boxes', 'cardinality']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489d7538-bd7b-455e-ad9c-aada55cd290b",
   "metadata": {},
   "source": [
    "#### Quick Illustration of How the Loss function get computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "2583f1b7-6739-47d7-bba2-ed7a04341da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/yuf/.cache/torch/hub/facebookresearch_detr_main\n"
     ]
    }
   ],
   "source": [
    "# initialize the pre-trained model\n",
    "model = DETRModel(num_classes, num_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "83f52144-e238-47d6-8ed8-a1b5e0c7da04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuf/anaconda3/lib/python3.7/site-packages/albumentations/augmentations/transforms.py:691: FutureWarning: This class has been deprecated. Please use CoarseDropout\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# from the 5 stratified data split, get 4 groups as training set\n",
    "fold=0\n",
    "df_train = df_folds[df_folds['fold'] != fold]\n",
    "train_dataset = WheatDataset(\n",
    "    image_ids=df_train.index.values,\n",
    "    dataframe=marking,\n",
    "    transforms=get_train_transforms()\n",
    "    )\n",
    "# For Mac users we unfortunately can not use parallel computing\n",
    "# num_workers should be set to 0\n",
    "train_data_loader = DataLoader(\n",
    "train_dataset,\n",
    "batch_size=BATCH_SIZE,\n",
    "shuffle=False,\n",
    "num_workers=0,\n",
    "collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "aff75ebc-8f3a-4b3a-8cce-d40305d89ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "633f4f86b6754e5f9df4e779fdfcf23e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/338 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'> <class 'tuple'> <class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "# lets take out one batch (here BATCH_SIZE=8)\n",
    "tk0 = tqdm(train_data_loader, total=len(train_data_loader))\n",
    "for step, (images, targets, image_ids) in enumerate(tk0):\n",
    "    # note that here images, targets\n",
    "    print(type(images), type(targets), type(image_ids))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "94aa3634-c0f7-44eb-b5cc-96977b7ba1a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we need to convert those tuple objects to lists\n",
    "\"Important observaiton: the criterion loss function takes two lists as Input!\"\n",
    "conv_images = list(image for image in images)\n",
    "conv_targets = [{k: v for k, v in t.items()} for t in targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "d4fb6ac0-69a0-486d-ac55-a0b638e20ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuf/.cache/torch/hub/facebookresearch_detr_main/models/position_encoding.py:41: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n"
     ]
    }
   ],
   "source": [
    "# the pre-trained detr model accepts list input\n",
    "conv_output = model(conv_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "5c346b0d-f805-497a-bf09-7654703a2701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 100, 2]), torch.Size([8, 100, 4]))"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output is a dictionary have 'pred_logits', 'pred_boxes' types, each have shape\n",
    "# (batch_size, num_queries, num_classes+1) and (batch_size, num_queries, 4)\n",
    "conv_output['pred_logits'].shape, conv_output['pred_boxes'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "a4e94163-4080-4c80-9ada-0b7393dc8cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this built in function compute altogether the loss for us, including Hungarian matching/class/bbox-distance/GIoU loss\n",
    "criterion = SetCriterion(num_classes-1, matcher, weight_dict, eos_coef = null_class_coef, losses=losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "f6963ca5-34fc-40e1-9199-d04f446a1a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss_ce': tensor(0.7234, grad_fn=<NllLoss2DBackward0>),\n",
       " 'class_error': tensor(10.0418),\n",
       " 'loss_bbox': tensor(0.4824, grad_fn=<DivBackward0>),\n",
       " 'loss_giou': tensor(1.0478, grad_fn=<DivBackward0>),\n",
       " 'cardinality_error': tensor(54.1250)}"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(conv_output, conv_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87dec69-879e-470a-a996-517ab9ad9009",
   "metadata": {},
   "source": [
    "#### Now the formal Training function & Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "3bd24e9c-b613-4a13-81d5-66e9f1bc2982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(data_loader,model,criterion,optimizer,device,scheduler,epoch):\n",
    "    model.train()\n",
    "    criterion.train()\n",
    "    \n",
    "    summary_loss = AverageMeter()\n",
    "    \n",
    "    tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "    \n",
    "    for step, (images, targets, image_ids) in enumerate(tk0):\n",
    "        \n",
    "        \"converting tensor to CUDA (GPU) enabled (to device)\"\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        output = model(images)\n",
    "        \"It seems that the criterion works here only for CUDA enabled tensors/values\"\n",
    "        loss_dict = criterion(output, targets)\n",
    "        weight_dict = criterion.weight_dict\n",
    "        \n",
    "        losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        summary_loss.update(losses.item(),BATCH_SIZE)\n",
    "        tk0.set_postfix(loss=summary_loss.avg)\n",
    "        \n",
    "    return summary_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "c7373d2e-7633-4fca-b897-70c02fb34622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn(data_loader, model,criterion, device):\n",
    "    model.eval()\n",
    "    criterion.eval()\n",
    "    summary_loss = AverageMeter()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "        for step, (images, targets, image_ids) in enumerate(tk0):\n",
    "            \n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            output = model(images)\n",
    "        \n",
    "            loss_dict = criterion(output, targets)\n",
    "            weight_dict = criterion.weight_dict\n",
    "        \n",
    "            losses = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n",
    "            \n",
    "            summary_loss.update(losses.item(),BATCH_SIZE)\n",
    "            tk0.set_postfix(loss=summary_loss.avg)\n",
    "    \n",
    "    return summary_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "c995430d-c038-4bc8-8ba3-3f4258784162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "891e9695-dad9-4851-97e3-7b8b24691293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(fold):\n",
    "    \n",
    "    df_train = df_folds[df_folds['fold'] != fold]\n",
    "    df_valid = df_folds[df_folds['fold'] == fold]\n",
    "    \n",
    "    train_dataset = WheatDataset(\n",
    "    image_ids=df_train.index.values,\n",
    "    dataframe=marking,\n",
    "    transforms=get_train_transforms()\n",
    "    )\n",
    "\n",
    "    valid_dataset = WheatDataset(\n",
    "    image_ids=df_valid.index.values,\n",
    "    dataframe=marking,\n",
    "    transforms=get_valid_transforms()\n",
    "    )\n",
    "    \n",
    "    train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    valid_data_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    #device = torch.device('cuda')\n",
    "    device = torch.device('cpu')\n",
    "    model = DETRModel(num_classes=num_classes,num_queries=num_queries)\n",
    "    model = model.to(device)\n",
    "    criterion = SetCriterion(num_classes-1, matcher, weight_dict, eos_coef = null_class_coef, losses=losses)\n",
    "    criterion = criterion.to(device)\n",
    "    \n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "    \n",
    "    best_loss = 10**5\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = train_fn(train_data_loader, model,criterion, optimizer,device,scheduler=None,epoch=epoch)\n",
    "        valid_loss = eval_fn(valid_data_loader, model,criterion, device)\n",
    "        \n",
    "        print('|EPOCH {}| TRAIN_LOSS {}| VALID_LOSS {}|'.format(epoch+1,train_loss.avg,valid_loss.avg))\n",
    "        \n",
    "        if valid_loss.avg < best_loss:\n",
    "            best_loss = valid_loss.avg\n",
    "            print('Best model found for Fold {} in Epoch {}........Saving Model'.format(fold,epoch+1))\n",
    "            torch.save(model.state_dict(), f'detr_best_{fold}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6199636-dab5-48c5-9729-12850987dde5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "run(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "302db415-aecd-45e0-9502-cdc84c87a343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_sample(df_valid,model,device):\n",
    "    '''\n",
    "    Code taken from Peter's Kernel \n",
    "    https://www.kaggle.com/pestipeti/pytorch-starter-fasterrcnn-train\n",
    "    '''\n",
    "    valid_dataset = WheatDataset(image_ids=df_valid.index.values,\n",
    "                                 dataframe=marking,\n",
    "                                 transforms=get_valid_transforms()\n",
    "                                )\n",
    "     \n",
    "    valid_data_loader = DataLoader(\n",
    "                                    valid_dataset,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    shuffle=False,\n",
    "                                   num_workers=0,\n",
    "                                   collate_fn=collate_fn)\n",
    "    \n",
    "    images, targets, image_ids = next(iter(valid_data_loader))\n",
    "    _,h,w = images[0].shape # for de normalizing images\n",
    "    \n",
    "    images = list(img.to(device) for img in images)\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    \n",
    "    boxes = targets[0]['boxes'].cpu().numpy()\n",
    "    boxes = [np.array(box).astype(np.int32) for box in A.augmentations.bbox_utils.denormalize_bboxes(boxes,h,w)]\n",
    "    sample = images[0].permute(1,2,0).cpu().numpy()\n",
    "    \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    cpu_device = torch.device(\"cpu\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        \n",
    "    outputs = [{k: v.to(cpu_device) for k, v in outputs.items()}]\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "\n",
    "    for box in boxes:\n",
    "        cv2.rectangle(sample,\n",
    "                  (box[0], box[1]),\n",
    "                  (box[2]+box[0], box[3]+box[1]),\n",
    "                  (220, 0, 0), 1)\n",
    "        \n",
    "\n",
    "    oboxes = outputs[0]['pred_boxes'][0].detach().cpu().numpy()\n",
    "    oboxes = [np.array(box).astype(np.int32) for box in A.augmentations.bbox_utils.denormalize_bboxes(oboxes,h,w)]\n",
    "    prob   = outputs[0]['pred_logits'][0].softmax(1).detach().cpu().numpy()[:,0]\n",
    "    \n",
    "    for box,p in zip(oboxes,prob):\n",
    "        \n",
    "        if p >0.5:\n",
    "            color = (0,0,220) #if p>0.5 else (0,0,0)\n",
    "            cv2.rectangle(sample,\n",
    "                  (box[0], box[1]),\n",
    "                  (box[2]+box[0], box[3]+box[1]),\n",
    "                  color, 1)\n",
    "    \n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7ccc4d-5685-414a-872b-114873b10b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DETRModel(num_classes=num_classes,num_queries=num_queries)\n",
    "model.load_state_dict(torch.load(\"./detr_best_0.pth\"))\n",
    "view_sample(df_folds[df_folds['fold'] == 0],model=model,device=torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d67d5e-d642-487a-a9e4-dfac0d46414f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
